# Take models generated by a particular algorithm on a set of
# sample data. Their performance on a second set of sample data is
# itself a random variable.
# It will vary, depending on the samples contained in both sets
# of sample data.
# We can see this variance in action with a simple example.
# Take an equiprobable binary random variable, such as the
# side a coin lands on when flipped. There is no pattern to
# the samples from this random variable, so models will have
# an expected long run performance of .5.
# Now imagine some models 'trained' to estimate the results
# of sampling from this random variable. They will produce sets
# of estimates for new data.
# When we 'train' enough of them, and then evaluate their performance
# on some new test data, some will do much better than .5 simply by
# virtue of this variance. Since we pick the best of these, we can
# expect it to perform much better than .5 on the validation data
# even though we know its long run performance will be .5.
# This can be summarized so: Because of variance and the fact we pick
# the best model, validation performance is a biased estimator of the
# performance of the selected model.
# To guard against such occurance, we test only the selected model
# again on a third, hold-out dataset.
# This will be an unbiased estimate of the selected model's long
# run performance.
# What should you do if your selected model performs much worse on
# the test data rather than the validation data?
# There are no good options: You need to start again, and find a
# new hold-out test dataset!
# n - number of models
coinFlippers=function(n=20) {
  cases=c("H","T")
  train=sample(cases,10,T)
  valid=sample(cases,10,T)
  test=sample(cases,10,T)

  models=lapply(1:n,function(i){
    list(valid=sample(cases,10,T),test=sample(cases,10,T))
  })

  valid_accuracy=sapply(models,function(model){
    length(which(model$valid==valid))/length(valid)
  })
  hist(valid_accuracy,main="Accuracy of models on validation data.",xlab="Validation Accuracy")

  ordered_valid_accuracy=order(valid_accuracy,decreasing=T)
  valid_accuracy[ordered_valid_accuracy]

  cat("The best model was model number",ordered_valid_accuracy[1],"with",valid_accuracy[ordered_valid_accuracy[1]],"accuracy.")

  test_accuracy=length(which(models[[ordered_valid_accuracy[1]]]$test==test))/length(test)

  cat("The best model had accuracy ",test_accuracy,"on the test data (vs",valid_accuracy[ordered_valid_accuracy[1]],"on the validation data).")
}

# We can see the bias in the performance of the selected
# model on validation and the unbiased nature of the 
# performance of the selected model on test data by
# looking at multiple runs.
coinFlippers2=function(m=1000,n=20) {
  res=sapply(1:m,function(run) {
    cases=c("H","T")
    train=sample(cases,10,T)
    valid=sample(cases,10,T)
    test=sample(cases,10,T)
    
    models=lapply(1:n,function(i){
      list(valid=sample(cases,10,T),test=sample(cases,10,T))
    })
    
    valid_accuracy=sapply(models,function(model){
      length(which(model$valid==valid))/length(valid)
    })
    ordered_valid_accuracy=order(valid_accuracy,decreasing=T)
    valid_accuracy[ordered_valid_accuracy]
    test_accuracy=length(which(models[[ordered_valid_accuracy[1]]]$test==test))/length(test)
    c(valid_accuracy[ordered_valid_accuracy[1]],test_accuracy)
  })
  
  par(mfrow=c(1,2))
  hist(res[1,],breaks=seq(0,1,.1),main=paste("Selected Model\nValidation Error\nMean:",mean(res[1,])),xlab="Correct")
  hist(res[2,],breaks=seq(0,1,.1),main=paste("Selected Model\nTest Error\nMean:",mean(res[2,])),xlab="Correct")
  par(mfrow=c(1,1))
}

